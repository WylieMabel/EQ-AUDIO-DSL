{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mabelwylie/Documents/EQ-AUDIO-DSL/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import seisbench.data as sbd\n",
    "import seisbench.generate as sbg\n",
    "import seisbench.models as sbm\n",
    "from seisbench.util import worker_seeding\n",
    "\n",
    "import obspy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy import UTCDateTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i want to load a seisbench data\n",
    "data = sbd.STEAD()\n",
    "mask = (data.metadata[\"trace_category\"] == \"earthquake_local\")\n",
    "data.filter(mask)\n",
    "train, dev, test = data.train_dev_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: DummyDataset - 60 traces\n",
      "Dev: DummyDataset - 10 traces\n",
      "Test: DummyDataset - 30 traces\n"
     ]
    }
   ],
   "source": [
    "data = sbd.DummyDataset(component_order = \"ZNE\")  # Reload to ensure we have the full dataset again\n",
    "\n",
    "\n",
    "train = data.train()\n",
    "dev = data.dev()\n",
    "test = data.test()\n",
    "\n",
    "print(\"Train:\", train)\n",
    "print(\"Dev:\", dev)\n",
    "print(\"Test:\", test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHANNEL_INDEX = {\"Z\":0, \"N\":1, \"E\":2}  # Channel order for ZNE\n",
    "CHANNEL = \"Z\"  # Channel to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: DummyDataset - 60 traces\n",
      "Dev: DummyDataset - 10 traces\n",
      "Test: DummyDataset - 30 traces\n",
      "\n",
      "--- Current Channel Configuration ---\n",
      "Selected Channel Index: Z (0)\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:00<00:00, 1345.72it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1387.56it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 1661.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Finished preprocessing all splits.\n",
      "Train size: 60, Dev size: 10, Test size: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import seisbench.data as sbd\n",
    "import numpy as np\n",
    "\n",
    "# ===========================================================\n",
    "# 1. Load Dataset\n",
    "# ===========================================================\n",
    "data = sbd.DummyDataset(component_order=\"ZNE\")\n",
    "\n",
    "train = data.train()\n",
    "dev = data.dev()\n",
    "test = data.test()\n",
    "\n",
    "print(\"Train:\", train)\n",
    "print(\"Dev:\", dev)\n",
    "print(\"Test:\", test)\n",
    "\n",
    "# ===========================================================\n",
    "# 2. Channel setup\n",
    "# ===========================================================\n",
    "CHANNEL_INDEX = {\"Z\": 0, \"N\": 1, \"E\": 2}\n",
    "CHANNEL = \"Z\"\n",
    "\n",
    "def preprocess_function(example):\n",
    "    \"\"\"\n",
    "    Extracts the desired channel waveform and attaches it as input_values.\n",
    "    \"\"\"\n",
    "    # SeisBench Trace object → waveform extraction\n",
    "\n",
    "    # Store as float32 for ML compatibility\n",
    "    #example[\"input_values\"] = example\n",
    "    #example[\"labels\"] = example[\"label\"]  # keep label as-is\n",
    "\n",
    "    return example\n",
    "\n",
    "print(f\"\\n--- Current Channel Configuration ---\")\n",
    "print(f\"Selected Channel Index: {CHANNEL} ({CHANNEL_INDEX[CHANNEL]})\")\n",
    "print(\"-----------------------------------\")\n",
    "\n",
    "# ===========================================================\n",
    "# 3. \"Mapping\" the preprocessing over SeisBench dataset\n",
    "# ===========================================================\n",
    "def map_seisbench(dataset, func):\n",
    "    \"\"\"\n",
    "    Apply a preprocessing function to each SeisBench example.\n",
    "    Returns a new SeisBench Dataset with modified examples.\n",
    "    \"\"\"\n",
    "    from tqdm import tqdm\n",
    "    processed = []\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        example = dataset.get_waveforms(i)[0,:]\n",
    "        processed.append(func(example))\n",
    "    return processed\n",
    "\n",
    "# Apply to all splits\n",
    "train_processed = map_seisbench(train, preprocess_function)\n",
    "dev_processed = map_seisbench(dev, preprocess_function)\n",
    "test_processed = map_seisbench(test, preprocess_function)\n",
    "\n",
    "print(\"\\n✅ Finished preprocessing all splits.\")\n",
    "print(f\"Train size: {len(train_processed)}, Dev size: {len(dev_processed)}, Test size: {len(test_processed)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 16000]) torch.Size([32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mabelwylie/Documents/EQ-AUDIO-DSL/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "CHANNEL_INDEX = {\"Z\": 0, \"N\": 1, \"E\": 2}\n",
    "CHANNEL = \"Z\"\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# 2. Define preprocessing function\n",
    "# ===========================================================\n",
    "def preprocess_function(trace,idx):\n",
    "    \"\"\"\n",
    "    Extracts the selected component and prepares for model input.\n",
    "    \"\"\"\n",
    "    single_channel = trace[CHANNEL_INDEX[CHANNEL], :]\n",
    "    # Convert to float32 tensor\n",
    "    x = torch.tensor(single_channel, dtype=torch.float32)\n",
    "    x_max = x.abs().max()\n",
    "    x_norm = x / x_max if x_max > 0 else x\n",
    "    x_resampled = torchaudio.transforms.Resample(orig_freq=1200, new_freq=16000)(x_norm)\n",
    "\n",
    "    y = torch.tensor(idx, dtype=torch.long)\n",
    "    return x_resampled, y\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# 3. Define custom PyTorch Dataset wrapper\n",
    "# ===========================================================\n",
    "class SeisbenchTorchDataset(Dataset):\n",
    "    def __init__(self, seisbench_dataset, transform=None, sampling_rate=100.0):\n",
    "        self.dataset = seisbench_dataset\n",
    "        self.transform = transform\n",
    "        self.sampling_rate = sampling_rate\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Lazy load only one trace from disk\n",
    "        trace = self.dataset.get_waveforms(idx)\n",
    "        if self.transform:\n",
    "            x, y = self.transform(trace, idx)\n",
    "        else:\n",
    "            x, y = trace, idx\n",
    "        return {\"input_values\": x, \"labels\": y}\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# 4. Instantiate datasets\n",
    "# ===========================================================\n",
    "train_dataset = SeisbenchTorchDataset(train, transform=preprocess_function)\n",
    "\n",
    "# ===========================================================\n",
    "# 5. Create DataLoader (lazy + parallel I/O)\n",
    "# ===========================================================\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0,     # parallel loading\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# ===========================================================\n",
    "# 6. Example usage\n",
    "# ===========================================================\n",
    "for batch in train_loader:\n",
    "    x = batch[\"input_values\"]  # shape: (batch_size, T)\n",
    "    y = batch[\"labels\"]\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mabelwylie/Documents/EQ-AUDIO-DSL/.venv/lib/python3.12/site-packages/transformers/configuration_utils.py:335: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output shape: torch.Size([32, 49, 768])\n"
     ]
    }
   ],
   "source": [
    "# run one batch through wav2vec2 model\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(x)\n",
    "    print(\"Model output shape:\", outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze feature extractor (conv front-end)\n",
    "for param in model.feature_extractor.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Freeze all encoder layers except the last 3\n",
    "for layer in model.encoder.layers[:-3]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 16000])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor, Wav2Vec2ForPreTraining\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import _compute_mask_indices, _sample_negative_indices\n",
    "\n",
    "model = Wav2Vec2ForPreTraining.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "\n",
    "# compute masked indices\n",
    "batch_size = 32\n",
    "raw_sequence_length = 16000\n",
    "sequence_length = model._get_feat_extract_output_lengths(raw_sequence_length).item()\n",
    "mask_time_indices = _compute_mask_indices(\n",
    "    shape=(batch_size, sequence_length), mask_prob=0.2, mask_length=2\n",
    ")\n",
    "sampled_negative_indices = _sample_negative_indices(\n",
    "    features_shape=(batch_size, sequence_length),\n",
    "    num_negatives=model.config.num_negatives,\n",
    "    mask_time_indices=mask_time_indices,\n",
    ")\n",
    "mask_time_indices = torch.tensor(data=mask_time_indices, device=input_values.device, dtype=torch.long)\n",
    "sampled_negative_indices = torch.tensor(\n",
    "    data=sampled_negative_indices, device=input_values.device, dtype=torch.long\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_values, mask_time_indices=mask_time_indices)\n",
    "\n",
    "# compute cosine similarity between predicted (=projected_states) and target (=projected_quantized_states)\n",
    "cosine_sim = torch.cosine_similarity(outputs.projected_states, outputs.projected_quantized_states, dim=-1)\n",
    "\n",
    "# show that cosine similarity is much higher than random\n",
    "cosine_sim[mask_time_indices.to(torch.bool)].mean() > 0.5\n",
    "tensor(True)\n",
    "\n",
    "# for contrastive loss training model should be put into train mode\n",
    "model = model.train()\n",
    "loss = model(\n",
    "    input_values, mask_time_indices=mask_time_indices, sampled_negative_indices=sampled_negative_indices\n",
    ").loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
